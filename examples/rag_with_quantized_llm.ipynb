{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1aeaea-43d9-476b-8fce-5c4b55707874",
   "metadata": {},
   "source": [
    "# Run Quantized LLMs on CPUs\n",
    "\n",
    "In this tutorial, we will demo how to build a RAG pipeline running on CPUs with [optimum-intel](https://github.com/huggingface/optimum-intel), using an LLM.\n",
    "\n",
    "We will use a pipeline that will:\n",
    "\n",
    "- Quantize an LLM\n",
    "- Fetch relevant documents for our question.\n",
    "- Rerank the documents for better performance.\n",
    "- Run the LLM on CPU to answer the question.\n",
    "\n",
    "For more information about optimum-intel, we refer to the [original repository](https://github.com/huggingface/optimum-intel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12b4ef-6626-4c2f-af4b-8023f93678b6",
   "metadata": {},
   "source": [
    "First, we export the model to an ONNX file:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:39:17.182162Z",
     "start_time": "2024-04-14T06:39:15.304411Z"
    }
   },
   "cell_type": "code",
   "source": "!ARCHFLAGS=\"-arch arm64\" pip install numpy  --compile --no-cache-dir",
   "id": "50df3c8e3588a237",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages (1.26.4)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T03:25:24.213042Z",
     "start_time": "2024-04-14T03:25:23.696768Z"
    }
   },
   "cell_type": "code",
   "source": "!pip freeze",
   "id": "2cdf4fbd6682c53a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate==0.29.2\r\n",
      "aiohttp==3.9.4\r\n",
      "aiosignal==1.3.1\r\n",
      "altair==5.3.0\r\n",
      "anyio==4.3.0\r\n",
      "appdirs==1.4.4\r\n",
      "appnope==0.1.4\r\n",
      "argon2-cffi==23.1.0\r\n",
      "argon2-cffi-bindings==21.2.0\r\n",
      "arrow==1.3.0\r\n",
      "asttokens==2.4.1\r\n",
      "async-lru==2.0.4\r\n",
      "async-timeout==4.0.3\r\n",
      "attrs==23.2.0\r\n",
      "Babel==2.14.0\r\n",
      "backoff==2.2.1\r\n",
      "beautifulsoup4==4.12.3\r\n",
      "bleach==6.1.0\r\n",
      "blinker==1.7.0\r\n",
      "blis==0.7.11\r\n",
      "boilerpy3==1.0.7\r\n",
      "cachetools==5.3.3\r\n",
      "catalogue==2.0.10\r\n",
      "cattrs==23.2.3\r\n",
      "certifi==2024.2.2\r\n",
      "cffi==1.16.0\r\n",
      "charset-normalizer==3.3.2\r\n",
      "click==8.1.7\r\n",
      "cloudpathlib==0.16.0\r\n",
      "comm==0.2.2\r\n",
      "confection==0.1.4\r\n",
      "contourpy==1.2.1\r\n",
      "cycler==0.12.1\r\n",
      "cymem==2.0.8\r\n",
      "datasets==2.18.0\r\n",
      "debugpy==1.8.1\r\n",
      "decorator==5.1.1\r\n",
      "defusedxml==0.7.1\r\n",
      "dill==0.3.8\r\n",
      "docopt==0.6.2\r\n",
      "et-xmlfile==1.1.0\r\n",
      "evaluate==0.4.1\r\n",
      "Events==0.5\r\n",
      "exceptiongroup==1.2.0\r\n",
      "executing==2.0.1\r\n",
      "farm-haystack==1.23.0\r\n",
      "fastapi==0.110.1\r\n",
      "fastjsonschema==2.19.1\r\n",
      "fastrag==2.0.0\r\n",
      "filelock==3.13.4\r\n",
      "fonttools==4.51.0\r\n",
      "fqdn==1.5.1\r\n",
      "frozenlist==1.4.1\r\n",
      "fsspec==2024.2.0\r\n",
      "gitdb==4.0.11\r\n",
      "GitPython==3.1.43\r\n",
      "h11==0.14.0\r\n",
      "htbuilder==0.6.2\r\n",
      "httpcore==1.0.5\r\n",
      "httpx==0.27.0\r\n",
      "huggingface-hub==0.22.2\r\n",
      "idna==3.7\r\n",
      "importlib_metadata==7.1.0\r\n",
      "importlib_resources==6.4.0\r\n",
      "inflect==7.2.0\r\n",
      "ipykernel==6.29.4\r\n",
      "ipython==8.18.1\r\n",
      "ipywidgets==8.1.2\r\n",
      "isoduration==20.11.0\r\n",
      "jedi==0.19.1\r\n",
      "Jinja2==3.1.3\r\n",
      "joblib==1.4.0\r\n",
      "json5==0.9.25\r\n",
      "jsonpickle==3.0.4\r\n",
      "jsonpointer==2.4\r\n",
      "jsonschema==4.21.1\r\n",
      "jsonschema-specifications==2023.12.1\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-console==6.6.3\r\n",
      "jupyter-events==0.10.0\r\n",
      "jupyter-lsp==2.2.5\r\n",
      "jupyter_client==8.6.1\r\n",
      "jupyter_core==5.7.2\r\n",
      "jupyter_server==2.14.0\r\n",
      "jupyter_server_terminals==0.5.3\r\n",
      "jupyterlab==4.1.6\r\n",
      "jupyterlab_pygments==0.3.0\r\n",
      "jupyterlab_server==2.26.0\r\n",
      "jupyterlab_widgets==3.0.10\r\n",
      "kiwisolver==1.4.5\r\n",
      "langcodes==3.3.0\r\n",
      "lazy-imports==0.3.1\r\n",
      "llvmlite==0.42.0\r\n",
      "Markdown==3.6\r\n",
      "markdown-it-py==3.0.0\r\n",
      "MarkupSafe==2.1.5\r\n",
      "matplotlib==3.8.4\r\n",
      "matplotlib-inline==0.1.6\r\n",
      "mdurl==0.1.2\r\n",
      "mistune==3.0.2\r\n",
      "monotonic==1.6\r\n",
      "more-itertools==10.2.0\r\n",
      "mpmath==1.3.0\r\n",
      "multidict==6.0.5\r\n",
      "multiprocess==0.70.16\r\n",
      "murmurhash==1.0.10\r\n",
      "nbclient==0.10.0\r\n",
      "nbconvert==7.16.3\r\n",
      "nbformat==5.10.4\r\n",
      "nest-asyncio==1.6.0\r\n",
      "networkx==3.2.1\r\n",
      "nltk==3.8.1\r\n",
      "notebook==7.1.2\r\n",
      "notebook_shim==0.2.4\r\n",
      "num2words==0.5.13\r\n",
      "numba==0.59.1\r\n",
      "numpy==1.26.4\r\n",
      "openpyxl==3.1.2\r\n",
      "overrides==7.7.0\r\n",
      "packaging==24.0\r\n",
      "pandas==2.2.2\r\n",
      "pandocfilters==1.5.1\r\n",
      "parso==0.8.4\r\n",
      "pexpect==4.9.0\r\n",
      "Pillow==10.1.0\r\n",
      "platformdirs==4.2.0\r\n",
      "posthog==3.5.0\r\n",
      "preshed==3.0.9\r\n",
      "prometheus_client==0.20.0\r\n",
      "prompt-toolkit==3.0.43\r\n",
      "prompthub-py==4.0.0\r\n",
      "protobuf==3.20.2\r\n",
      "psutil==5.9.8\r\n",
      "ptyprocess==0.7.0\r\n",
      "pure-eval==0.2.2\r\n",
      "pyarrow==15.0.2\r\n",
      "pyarrow-hotfix==0.6\r\n",
      "pycparser==2.22\r\n",
      "pydantic==1.10.15\r\n",
      "pydeck==0.8.1b0\r\n",
      "Pygments==2.17.2\r\n",
      "pyparsing==3.1.2\r\n",
      "python-dateutil==2.9.0.post0\r\n",
      "python-json-logger==2.0.7\r\n",
      "pytz==2024.1\r\n",
      "pyvis==0.3.2\r\n",
      "PyYAML==6.0.1\r\n",
      "pyzmq==25.1.2\r\n",
      "qtconsole==5.5.1\r\n",
      "QtPy==2.4.1\r\n",
      "quantulum3==0.9.0\r\n",
      "rank-bm25==0.2.2\r\n",
      "referencing==0.34.0\r\n",
      "regex==2023.12.25\r\n",
      "requests==2.31.0\r\n",
      "requests-cache==0.9.8\r\n",
      "responses==0.18.0\r\n",
      "rfc3339-validator==0.1.4\r\n",
      "rfc3986-validator==0.1.1\r\n",
      "rich==13.7.1\r\n",
      "rpds-py==0.18.0\r\n",
      "safetensors==0.4.2\r\n",
      "scikit-learn==1.4.2\r\n",
      "scipy==1.13.0\r\n",
      "Send2Trash==1.8.3\r\n",
      "sentence-transformers==2.6.1\r\n",
      "sentencepiece==0.2.0\r\n",
      "six==1.16.0\r\n",
      "smart-open==6.4.0\r\n",
      "smmap==5.0.1\r\n",
      "sniffio==1.3.1\r\n",
      "soupsieve==2.5\r\n",
      "spacy==3.7.4\r\n",
      "spacy-legacy==3.0.12\r\n",
      "spacy-loggers==1.0.5\r\n",
      "srsly==2.4.8\r\n",
      "sseclient-py==1.8.0\r\n",
      "st-annotated-text==4.0.1\r\n",
      "stack-data==0.6.3\r\n",
      "starlette==0.37.2\r\n",
      "streamlit==1.33.0\r\n",
      "sympy==1.12\r\n",
      "tenacity==8.2.3\r\n",
      "terminado==0.18.1\r\n",
      "thinc==8.2.3\r\n",
      "threadpoolctl==3.4.0\r\n",
      "tiktoken==0.6.0\r\n",
      "tinycss2==1.2.1\r\n",
      "tokenizers==0.15.2\r\n",
      "toml==0.10.2\r\n",
      "tomli==2.0.1\r\n",
      "toolz==0.12.1\r\n",
      "torch==2.2.2\r\n",
      "tornado==6.4\r\n",
      "tqdm==4.66.2\r\n",
      "traitlets==5.14.2\r\n",
      "transformers==4.35.2\r\n",
      "typeguard==4.2.1\r\n",
      "typer==0.9.4\r\n",
      "types-python-dateutil==2.9.0.20240316\r\n",
      "typing_extensions==4.11.0\r\n",
      "tzdata==2024.1\r\n",
      "ujson==5.9.0\r\n",
      "uri-template==1.3.0\r\n",
      "url-normalize==1.4.3\r\n",
      "urllib3==2.2.1\r\n",
      "uvicorn==0.29.0\r\n",
      "wasabi==1.1.2\r\n",
      "wcwidth==0.2.13\r\n",
      "weasel==0.3.4\r\n",
      "webcolors==1.13\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client==1.7.0\r\n",
      "widgetsnbextension==4.0.10\r\n",
      "xxhash==3.4.1\r\n",
      "yarl==1.9.4\r\n",
      "zipp==3.18.1\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:39:21.217540Z",
     "start_time": "2024-04-14T06:39:21.214531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'facebook/opt-iml-max-1.3b'\n",
    "\n",
    "converted_model_path = f\"/tmp/{model_name.replace('/','_')}\""
   ],
   "id": "d92d7e09-c09e-484a-a569-f91339f9875f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "96a05063-37da-49dd-ae65-9662bd90c4fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:40:47.341012Z",
     "start_time": "2024-04-14T06:39:21.543342Z"
    }
   },
   "source": [
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "\n",
    "model = ORTModelForCausalLM.from_pretrained(model_name, export=True, trust_remote_code=True)\n",
    "model.save_pretrained(converted_model_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Framework not specified. Using pt to export the model.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.2.2\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/models/opt/modeling_opt.py:586: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif attention_mask.shape[1] != mask_seq_length:\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:94: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/models/opt/modeling_opt.py:181: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/models/opt/modeling_opt.py:188: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/models/opt/modeling_opt.py:194: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n",
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/models/opt/modeling_opt.py:227: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Saving external data to one file...\n",
      "Post-processing the exported models...\n",
      "Deduplicating shared (tied) weights...\n",
      "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
      "\tlm_head.weight: {'onnx::MatMul_5595'}\n",
      "\tmodel.decoder.embed_tokens.weight: {'model.decoder.embed_tokens.weight'}\n",
      "Removing duplicate initializer onnx::MatMul_5595...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "cea3cd81-5080-4796-9f4b-d9c6f6172913",
   "metadata": {},
   "source": [
    "Then, we load the exported model back, and quantize it:"
   ]
  },
  {
   "cell_type": "code",
   "id": "6326e749-49df-4d16-8be1-fb5f3af4b603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:50.388802Z",
     "start_time": "2024-04-14T06:40:47.350398Z"
    }
   },
   "source": [
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "import os\n",
    "\n",
    "model = ORTModelForCausalLM.from_pretrained(converted_model_path)\n",
    "\n",
    "qconfig = AutoQuantizationConfig.avx2(is_static=False)\n",
    "quantizer = ORTQuantizer.from_pretrained(model)\n",
    "\n",
    "quantizer.quantize(save_dir=os.path.join(converted_model_path, 'quantized'), quantization_config=qconfig)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/u8, channel-wise: True)\n",
      "Quantizing model...\n",
      "Saving quantized model at: /tmp/facebook_opt-iml-max-1.3b/quantized (external data format: False)\n",
      "Configuration saved in /tmp/facebook_opt-iml-max-1.3b/quantized/ort_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/tmp/facebook_opt-iml-max-1.3b/quantized')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "1fb51e32-c066-418c-a021-73a3e3f13f1f",
   "metadata": {},
   "source": [
    "Now that our model is quantized, we can create a RAG pipeline with it:"
   ]
  },
  {
   "cell_type": "code",
   "id": "c25d1abb-1e57-4290-8a6e-43aa20ceca17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:52.117304Z",
     "start_time": "2024-04-14T06:45:50.391322Z"
    }
   },
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.nodes.prompt import PromptNode\n",
    "import torch\n",
    "from haystack.nodes import PromptModel\n",
    "from haystack.nodes.prompt.prompt_template import PromptTemplate\n",
    "from haystack.nodes import AnswerParser\n",
    "from haystack.nodes.ranker import SentenceTransformersRanker\n",
    "from haystack.nodes.retriever import BM25Retriever\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack import Document"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "0d1cfe9c-7861-4ed8-836e-689a612faf15",
   "metadata": {},
   "source": [
    "We start from a collection of paragraphs from Wikipedia, for the retrieval phase:"
   ]
  },
  {
   "cell_type": "code",
   "id": "23442f1b-ede3-4ef8-9768-d0e598315cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:52.123695Z",
     "start_time": "2024-04-14T06:45:52.119577Z"
    }
   },
   "source": [
    "document_collection = [{'id': '11457596',\n",
    "  'text': 'Quest\", the \"Ultima\" series, \"EverQuest\", the \"Warcraft\" series, and the \"Elder Scrolls\" series of games as well as video games set in Middle-earth itself. Research also suggests that some consumers of fantasy games derive their motivation from trying to create an epic fantasy narrative which is influenced by \"The Lord of the Rings\". In 1965, songwriter Donald Swann, who was best known for his collaboration with Michael Flanders as Flanders & Swann, set six poems from \"The Lord of the Rings\" and one from \"The Adventures of Tom Bombadil\" (\"Errantry\") to music. When Swann met with Tolkien to play the',\n",
    "  'title': 'The Lord of the Rings'},\n",
    " {'id': '11457582',\n",
    "  'text': 'helped \"The Lord of the Rings\" become immensely popular in the United States in the 1960s. The book has remained so ever since, ranking as one of the most popular works of fiction of the twentieth century, judged by both sales and reader surveys. In the 2003 \"Big Read\" survey conducted in Britain by the BBC, \"The Lord of the Rings\" was found to be the \"Nation\\'s best-loved book\". In similar 2004 polls both Germany and Australia also found \"The Lord of the Rings\" to be their favourite book. In a 1999 poll of Amazon.com customers, \"The Lord of the',\n",
    "  'title': 'The Lord of the Rings'},\n",
    " {'id': '11457540',\n",
    "  'text': 'of Tolkien\\'s works is such that the use of the words \"Tolkienian\" and \"Tolkienesque\" has been recorded in the \"Oxford English Dictionary\". The enduring popularity of \"The Lord of the Rings\" has led to numerous references in popular culture, the founding of many societies by fans of Tolkien\\'s works, and the publication of many books about Tolkien and his works. \"The Lord of the Rings\" has inspired, and continues to inspire, artwork, music, films and television, video games, board games, and subsequent literature. Award-winning adaptations of \"The Lord of the Rings\" have been made for radio, theatre, and film. In',\n",
    "  'title': 'The Lord of the Rings'},\n",
    " {'id': '11457587',\n",
    "  'text': 'has been read as fitting the model of Joseph Campbell\\'s \"monomyth\". \"The Lord of the Rings\" has been adapted for film, radio and stage. The book has been adapted for radio four times. In 1955 and 1956, the BBC broadcast \"The Lord of the Rings\", a 13-part radio adaptation of the story. In the 1960s radio station WBAI produced a short radio adaptation. A 1979 dramatization of \"The Lord of the Rings\" was broadcast in the United States and subsequently issued on tape and CD. In 1981, the BBC broadcast \"The Lord of the Rings\", a new dramatization in 26',\n",
    "  'title': 'The Lord of the Rings'},\n",
    " {'id': '11457592',\n",
    "  'text': '\"The Lord of the Rings\", was released on the internet in May 2009 and has been covered in major media. \"Born of Hope\", written by Paula DiSante, directed by Kate Madison, and released in December 2009, is a fan film based upon the appendices of \"The Lord of the Rings\". In November 2017, Amazon acquired the global television rights to \"The Lord of the Rings\", committing to a multi-season television series. The series will not be a direct adaptation of the books, but will instead introduce new stories that are set before \"The Fellowship of the Ring\". Amazon said the',\n",
    "  'title': 'The Lord of the Rings'},\n",
    " {'id': '7733817',\n",
    "  'text': 'The Lord of the Rings Online The Lord of the Rings Online: Shadows of Angmar is a massive multiplayer online role-playing game (MMORPG) for Microsoft Windows and OS X set in a fantasy universe based upon J. R. R. Tolkien\\'s Middle-earth writings, taking place during the time period of \"The Lord of the Rings\". It launched in North America, Australia, Japan, and Europe in 2007. Originally subscription-based, it is free-to-play, with a paid VIP subscription available that provides players various perks.  The game\\'s environment is based on \"The Lord of the Rings\" and \"The Hobbit\". However, Turbine does not',\n",
    "  'title': 'The Lord of the Rings Online'},\n",
    " {'id': '22198847',\n",
    "  'text': 'of \"The Lord of the Rings\", including Ian McKellen, Andy Serkis, Hugo Weaving, Elijah Wood, Ian Holm, Christopher Lee, Cate Blanchett and Orlando Bloom who reprised their roles. Although the \"Hobbit\" films were even more commercially successful than \"The Lord of the Rings\", they received mixed reviews from critics. Numerous video games were released to supplement the film series. They include: \",\" Pinball, \"\", \"\", , \"\", \"\", \"\", \"\", \"The Lord of the Rings Online\", \"\", \"\", \"\", \"Lego The Lord of the Rings\", \"Guardians of Middle-earth\", \"\", and \"\".',\n",
    "  'title': 'The Lord of the Rings (film series)'},\n",
    " {'id': '24071573',\n",
    "  'text': 'Lord of the Rings (musical) The Lord of the Rings is the most prominent of several theatre adaptations of J. R. R. Tolkien\\'s epic high fantasy novel of the same name, with music by A. R. Rahman, Christopher Nightingale and the band Värttinä, and book and lyrics by Matthew Warchus and Shaun McKenna. Set in the world of Middle-earth, \"The Lord of the Rings\" tells the tale of a humble hobbit who is asked to play the hero and undertake a treacherous mission to destroy an evil, magic ring without being seduced by its power. The show was first performed',\n",
    "  'title': 'Lord of the Rings (musical)'},\n",
    " {'id': '11457536',\n",
    "  'text': 'The Lord of the Rings The Lord of the Rings is an epic high fantasy novel written by English author and scholar J. R. R. Tolkien. The story began as a sequel to Tolkien\\'s 1937 fantasy novel \"The Hobbit\", but eventually developed into a much larger work. Written in stages between 1937 and 1949, \"The Lord of the Rings\" is one of the best-selling novels ever written, with over 150 million copies sold. The title of the novel refers to the story\\'s main antagonist, the Dark Lord Sauron, who had in an earlier age created the One Ring to rule',\n",
    "  'title': 'The Lord of the Rings'},\n",
    " {'id': '13304003',\n",
    "  'text': \"The Lord of the Rings (disambiguation) The Lord of the Rings is a fantasy novel by J. R. R. Tolkien. The title refers to Sauron, the story's main antagonist. The Lord of the Rings may also refer to:\",\n",
    "  'title': 'The Lord of the Rings (disambiguation)'}]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "6d381c2c-a3ca-4bb8-b003-481d6aed5148",
   "metadata": {},
   "source": [
    "We then create an InMemoryDocumentStore document store, to store all the documents:"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8f3e6df-3efa-41c7-a096-11f90b4b9c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:52.127196Z",
     "start_time": "2024-04-14T06:45:52.124357Z"
    }
   },
   "source": [
    "store = InMemoryDocumentStore(use_bm25=True)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "99700ae4-fd3f-4d16-ad7c-8cd746c9999d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:52.130099Z",
     "start_time": "2024-04-14T06:45:52.127738Z"
    }
   },
   "source": [
    "documents = [Document(id=item[\"id\"], content=item[\"text\"], meta={\"title\": item[\"title\"]}) for item in document_collection]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "a2d901a6-a350-45a9-b69e-ab44a4dad9f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:52.161633Z",
     "start_time": "2024-04-14T06:45:52.130834Z"
    }
   },
   "source": [
    "store.write_documents(documents)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating BM25 representation...: 100%|██████████| 10/10 [00:00<00:00, 8977.53 docs/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "beb6fb8f-eb34-421c-9ceb-32f8df00ec6b",
   "metadata": {},
   "source": [
    "Next, we create a simple BM25 retriever on top of our store, and an additional reranker component to improve the ranking of the documents used for answering the question:"
   ]
  },
  {
   "cell_type": "code",
   "id": "702ce4e4-6a26-473f-9804-91136ddb60e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:54.319399Z",
     "start_time": "2024-04-14T06:45:52.163635Z"
    }
   },
   "source": [
    "\n",
    "retriever = BM25Retriever(\n",
    "    document_store= store,\n",
    "    top_k= 10\n",
    ")    \n",
    " \n",
    "reranker = SentenceTransformersRanker(\n",
    "    batch_size= 32,\n",
    "    model_name_or_path= \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_k= 1,\n",
    "    use_gpu= False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "b5334ce5-48ff-4f09-804e-45bf96e8c2b2",
   "metadata": {},
   "source": [
    "Now that we have created the retrieval components, we move to the LLM usage.\n",
    "\n",
    "We create a document template that contains a placeholder for the retrieved documents to be inserted:"
   ]
  },
  {
   "cell_type": "code",
   "id": "343eae7e-9c83-4d0e-9b60-706c573466be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:54.323847Z",
     "start_time": "2024-04-14T06:45:54.320248Z"
    }
   },
   "source": [
    "AParser = AnswerParser()\n",
    "LFQA = PromptTemplate(\n",
    "    prompt=\"\"\"Answer the Question below using only the Document provided.\n",
    "Do not use any prior knowledge to answer the question.\n",
    "Your answer can only be an entity name or a short phrase.\n",
    "\n",
    "Document:\n",
    "{join(documents)}\n",
    "\n",
    "Question: {query}\n",
    "Answer: \"\"\",\n",
    "    output_parser= AParser\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "f1713866-5fb0-430a-bf8d-e879c2dc9905",
   "metadata": {},
   "source": [
    "We now create a PromptModel with an LLM, using the ```ORTInvocationLayer``` class, to load the LLM as a quantized ONNX model on our CPU."
   ]
  },
  {
   "cell_type": "code",
   "id": "5706b86d-4393-466e-a578-d773683bc626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:54.757044Z",
     "start_time": "2024-04-14T06:45:54.326427Z"
    }
   },
   "source": [
    "from fastrag.prompters.invocation_layers.ort import ORTInvocationLayer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linhnguyen/Documents/Work/repo/fastRAG/venv-2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "800029d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "37016502-d9f3-4084-8e8f-aac90f7a0831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:54.759729Z",
     "start_time": "2024-04-14T06:45:54.757740Z"
    }
   },
   "source": [
    "import os"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "0238a927-736c-498d-afd0-428971daa175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:57.412343Z",
     "start_time": "2024-04-14T06:45:54.760461Z"
    }
   },
   "source": [
    "prompter_model = PromptModel(\n",
    "    model_name_or_path= os.path.join(converted_model_path, 'quantized'),\n",
    "    invocation_layer_class=ORTInvocationLayer,\n",
    "    model_kwargs= dict(\n",
    "        task_name=\"text-generation\",\n",
    "        max_new_tokens=20,\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "5e9a00ea-e59b-4492-b175-eaef76f33c07",
   "metadata": {},
   "source": [
    "With the model and the prompt template now ready, we create a PromptNode to unify both modules:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e07a9488-5562-459f-9624-5aaefd66aeaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:57.415176Z",
     "start_time": "2024-04-14T06:45:57.413088Z"
    }
   },
   "source": [
    "Prompter = PromptNode(\n",
    "    model_name_or_path= prompter_model,\n",
    "    default_prompt_template= LFQA\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "e03a9423-fcaa-4c2a-b677-f32d79f642ce",
   "metadata": {},
   "source": [
    "Our components are now ready. We can now create a pipeline, to connect all of them together:"
   ]
  },
  {
   "cell_type": "code",
   "id": "6fa7057c-5e29-496d-8330-c8d9db58936f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:57.418161Z",
     "start_time": "2024-04-14T06:45:57.415961Z"
    }
   },
   "source": [
    "from haystack import Pipeline"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "7d4a517e-9fa6-407f-9527-e372e1623c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:45:57.423627Z",
     "start_time": "2024-04-14T06:45:57.419441Z"
    }
   },
   "source": [
    "pipe = Pipeline()\n",
    "\n",
    "pipe.add_node(component=retriever, name= 'Retriever',inputs= [\"Query\"])\n",
    "pipe.add_node(component=reranker, name= 'Reranker',inputs= [\"Retriever\"])\n",
    "pipe.add_node(component=Prompter, name= 'Prompter',inputs= [\"Reranker\"])"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "ae3804a9-a067-4033-bded-3a76c7294be1",
   "metadata": {},
   "source": [
    "Finally, lets ask it a question."
   ]
  },
  {
   "cell_type": "code",
   "id": "3c0206f4-adf6-4c0d-bd7b-be5fa9851f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:52:15.651868Z",
     "start_time": "2024-04-14T06:52:13.573348Z"
    }
   },
   "source": [
    "answer_result = pipe.run(\"Who is the main villan in Lord of the Rings?\",params={\n",
    "    \"Retriever\": {\n",
    "        \"top_k\": 10\n",
    "    },\n",
    "    \"Reranker\": {\n",
    "        \"top_k\": 1\n",
    "    },\n",
    "    \"generation_kwargs\":{\n",
    "        \"max_length\": 10,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "})\n",
    "\n",
    "print(f\"Answer: {answer_result['answers'][0].answer}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Sauron\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:52:27.182612Z",
     "start_time": "2024-04-14T06:52:25.133126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer_result = pipe.run(\"What crimes did Sauron commit\",params={\n",
    "    \"Retriever\": {\n",
    "        \"top_k\": 10\n",
    "    },\n",
    "    \"Reranker\": {\n",
    "        \"top_k\": 1\n",
    "    },\n",
    "    \"generation_kwargs\":{\n",
    "        \"max_length\": 10,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "})\n",
    "\n",
    "print(f\"Answer: {answer_result['answers'][0].answer}\")"
   ],
   "id": "48e0286aef250bf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
